# -*- coding: utf-8 -*-
"""QNN_0904_2118.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RmRUe5m3GCuHzMEIs-raZnKMZKf2UR-C
"""

!pip install torch torchvision torchaudio tqdm -q

# --- Import libraries ---
import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
from torch.utils.data import DataLoader
import os
import time
import copy
import numpy as np
from tqdm import tqdm
import pandas as pd

# --- GPU / CPU configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"사용 디바이스: {device}")
print(f"PyTorch version: {torch.__version__}")
print(f"NumPy version: {np.__version__}")

# --- Kaggle API setup (for downloading dataset) ---
# Upload kaggle.json for authentication
from google.colab import files
if not os.path.exists("kaggle.json"):
    files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# --- Download and extract ImageNet-mini dataset ---
if not os.path.exists("/content/imagenet-mini"):
    print("Downloading ImageNet-mini dataset...")
    !kaggle datasets download -d ifigotin/imagenetmini-1000
    !unzip -q imagenetmini-1000.zip -d /content/imagenet-mini
    print("Download and extraction completed.")
else:
    print("ImageNet-mini dataset already exists.")

# --- Data loader setup ---
data_dir = "/content/imagenet-mini/imagenet-mini"
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
val_dataset = torchvision.datasets.ImageFolder(os.path.join(data_dir, "val"), transform=transform)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2) # batch size increased

print(f"\nNumber of validation images: {len(val_dataset)}")
print(f"Number of classes: {len(val_dataset.classes)}")

# --- Evaluation function: computes Top-1, Top-5 accuracy and inference time ---
def evaluate_model(model, dataloader, device):
    model.to(device)
    model.eval()
    correct_top1, correct_top5, total = 0, 0, 0
    start_time = time.time()
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Evaluating", ncols=80):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)

            # Compute Top-5 predictions
            _, pred = outputs.topk(5, 1, True, True)
            pred = pred.t()
            correct = pred.eq(labels.view(1, -1).expand_as(pred))

            # Count Top-1 accuracy
            correct_top1 += correct[:1].reshape(-1).float().sum(0, keepdim=True).item()
            # Count Top-5 accuracy
            correct_top5 += correct[:5].reshape(-1).float().sum(0, keepdim=True).item()
            total += labels.size(0)

    elapsed = time.time() - start_time
    top1_acc = (correct_top1 / total) * 100
    top5_acc = (correct_top5 / total) * 100
    return top1_acc, top5_acc, elapsed

# --- Function to measure model size (in MB) ---
def get_model_size(model, file_path="temp_model.pth"):
    torch.save(model.state_dict(), file_path)       # Save model parameters
    size_mb = os.path.getsize(file_path) / (1024 * 1024)
    os.remove(file_path)
    return size_mb

# Dictionary to store all results
results = {}

# ==============================================================================
# Step 1: Evaluate Pretrained FP32 ResNet-50 (Baseline on CPU)
# ==============================================================================
print("\n--- Step 1: FP32 ResNet-50 evaluation ---")
fp32_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)  # Load FP32 pretrained model

fp32_size = get_model_size(fp32_model)
fp32_top1, fp32_top5, fp32_time = evaluate_model(fp32_model, val_loader, "cpu")

results['FP32'] = {
    'Model Size (MB)': fp32_size,
    'Top-1 Acc (%)': fp32_top1,
    'Top-5 Acc (%)': fp32_top5,
    'Inference Time (s)': fp32_time
}

print(f"\n[FP32] Model size: {fp32_size:.2f} MB")
print(f"[FP32] Top-1 Accuracy: {fp32_top1:.2f}%")
print(f"[FP32] Top-5 Accuracy: {fp32_top5:.2f}%")
print(f"[FP32] Inference time (CPU): {fp32_time:.2f} s")

# ==============================================================================
# Step 2: Evaluate Pretrained INT8 ResNet-50 (Quantized Model)
# ==============================================================================
print("\n--- Step 2: Pretrained INT8 ResNet-50 evaluation ---")
int8_model = torchvision.models.quantization.resnet50(pretrained=True, quantize=True)  # Load quantized INT8 model
int8_model.eval()

int8_size = get_model_size(int8_model)
int8_top1, int8_top5, int8_time = evaluate_model(int8_model, val_loader, "cpu")

results['Pretrained INT8'] = {
    'Model Size (MB)': int8_size,
    'Top-1 Acc (%)': int8_top1,
    'Top-5 Acc (%)': int8_top5,
    'Inference Time (s)': int8_time
}

    print(f"\n[Pretrained INT8] Model size: {int8_size:.2f} MB")
    print(f"[Pretrained INT8] Top-1 Accuracy: {int8_top1:.2f}%")
    print(f"[Pretrained INT8] Top-5 Accuracy: {int8_top5:.2f}%")
    print(f"[Pretrained INT8] Inference time (CPU): {int8_time:.2f} s")


# ==============================================================================
# Step 3: Manual INT8 Quantization (Naive implementation with scale)
# ==============================================================================
print("\n--- Step 3: Manual INT8 quantization and evaluation ---")

# Quantization: map float tensor → int8 tensor using max-abs scaling
def quantize_tensor(x):
    alpha = torch.max(torch.abs(x))        # max absolute value
    scale = alpha / 127.0                  # scaling factor
    x_q = torch.round(x / scale).to(torch.int8)
    return x_q, scale

# Dequantization: map int8 tensor → float tensor
def dequantize_tensor(x_q, scale):
    return x_q.float() * scale

manual_quant_model = copy.deepcopy(fp32_model).to("cpu")
quantized_state_dict = {}
scales = {}

# Apply quantization to weights
for name, param in manual_quant_model.state_dict().items():
    if 'weight' in name and param.dim() > 1:   # only quantize weight tensors
        quantized_param, scale = quantize_tensor(param)
        quantized_state_dict[name] = quantized_param
        scales[name] = scale
    else:
        quantized_state_dict[name] = param

# Estimate model size: count int8 params (1 byte each) and scales (float32)
quant_params_size = 0
float_params_size = 0
for name, param in quantized_state_dict.items():
    if param.dtype == torch.int8:
        quant_params_size += param.numel() * 1   # int8 → 1 byte
        float_params_size += 4                  # store scale (1 float32 per tensor)
    else:
        float_params_size += param.numel() * 4
manual_quant_size = (quant_params_size + float_params_size) / (1024*1024)

# Reconstruct model with dequantized parameters for evaluation
dequantized_state_dict = {}
for name, param in quantized_state_dict.items():
    if name in scales:
        dequantized_state_dict[name] = dequantize_tensor(param, scales[name])
    else:
        dequantized_state_dict[name] = param

eval_model = copy.deepcopy(fp32_model).to("cpu")
eval_model.load_state_dict(dequantized_state_dict)

manual_quant_top1, manual_quant_top5, manual_quant_time = evaluate_model(eval_model, val_loader, "cpu")

results['Manual INT8'] = {
    'Model Size (MB)': manual_quant_size,
    'Top-1 Acc (%)': manual_quant_top1,
    'Top-5 Acc (%)': manual_quant_top5,
    'Inference Time (s)': manual_quant_time
}

print(f"\n[Manual INT8] Model size: {manual_quant_size:.2f} MB")
print(f"[Manual INT8] Top-1 Accuracy: {manual_quant_top1:.2f}%")
print(f"[Manual INT8] Top-5 Accuracy: {manual_quant_top5:.2f}%")
print(f"[Manual INT8] Inference time (CPU): {manual_quant_time:.2f} s")

# ==============================================================================
# Step 4: Final results comparison (table)
# ==============================================================================
import pandas as pd
from IPython.display import display, Markdown 

print("\n--- Final Results Comparison ---\n")

df = pd.DataFrame(results).T.round(2)

display(Markdown("### Table 1: Model Performance Comparison"))
display(df)
