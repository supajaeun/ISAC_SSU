# -*- coding: utf-8 -*-
"""QNN_0913.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sXS5dRK3QrAoP1ftjpUGh9-ngb-vyKmA
"""

!pip uninstall torch torchvision torchaudio -y
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

!pip show torch
!pip show torchvision

# --- Import libraries ---
import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
from torch.utils.data import DataLoader
import os
import time
import copy
import numpy as np
from tqdm import tqdm
import pandas as pd

print(torch.__version__)
print(torch.backends.quantized.engine)

# --- Kaggle API setup (kaggle.json upload required) ---
# After running this cell, upload kaggle.json using the 'Choose File' button
from google.colab import files
if not os.path.exists("kaggle.json"):
    files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# --- Download and extract ImageNet-mini dataset ---
if not os.path.exists("/content/imagenet-mini"):
    print("Starting ImageNet-mini dataset download...")
    !kaggle datasets download -d ifigotin/imagenetmini-1000
    !unzip -q imagenetmini-1000.zip -d /content/imagenet-mini
    print("Download and extraction completed.")
else:
    print("ImageNet-mini dataset already exists.")

# --- Data loader setup ---
data_dir = "/content/imagenet-mini/imagenet-mini"
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
val_dataset = torchvision.datasets.ImageFolder(os.path.join(data_dir, "val"), transform=transform)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)  # Increased batch size

print(f"\nValidation image count: {len(val_dataset)}")
print(f"Number of classes: {len(val_dataset.classes)}")

# Evaluation function
def evaluate_model(model, dataloader, device):
    model.to(device)
    model.eval()
    correct_top1, correct_top5, total = 0, 0, 0
    start_time = time.time()
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Evaluating", ncols=80):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)

            # Calculate Top-5 predictions
            _, pred = outputs.topk(5, 1, True, True)
            pred = pred.t()
            correct = pred.eq(labels.view(1, -1).expand_as(pred))

            # Top-1 accuracy
            correct_top1 += correct[:1].reshape(-1).float().sum(0, keepdim=True).item()
            # Top-5 accuracy
            correct_top5 += correct[:5].reshape(-1).float().sum(0, keepdim=True).item()
            total += labels.size(0)

    elapsed = time.time() - start_time
    top1_acc = (correct_top1 / total) * 100
    top5_acc = (correct_top5 / total) * 100
    return top1_acc, top5_acc, elapsed

# --- Common model size measurement function ---
def get_model_size(model, file_path="temp_model.pth"):
    torch.save(model.state_dict(), file_path)
    size_mb = os.path.getsize(file_path) / (1024 * 1024)
    os.remove(file_path)
    return size_mb

# Dictionary to store results
results = {}

# ==============================================================================
# Step 1: Evaluate Pretrained FP32 ResNet-50 (Baseline on CPU)
# ==============================================================================
print("\n--- Step 1: Start evaluating FP32 ResNet-50 ---")
fp32_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)

fp32_size = get_model_size(fp32_model)

fp32_top1, fp32_top5, fp32_time = evaluate_model(fp32_model, val_loader, "cpu")

results['FP32'] = {'Model Size (MB)': fp32_size, 'Top-1 Acc (%)': fp32_top1, 'Top-5 Acc (%)': fp32_top5, 'Inference Time (s)': fp32_time}

print(f"\n[FP32] Model size: {fp32_size:.2f} MB")
print(f"[FP32] Top-1 Accuracy: {fp32_top1:.2f}%")
print(f"[FP32] Top-5 Accuracy: {fp32_top5:.2f}%")
print(f"[FP32] Evaluation time (CPU): {fp32_time:.2f} s")

# ==============================================================================
# Step 2: Evaluate Pretrained INT8 ResNet-50 (Quantized Model)
# ==============================================================================
print("\n--- Step 2: Start evaluating Pretrained INT8 ResNet-50 ---")
try:
    int8_model = torchvision.models.quantization.resnet50(pretrained=True, quantize=True)
    int8_model.eval()

    int8_size = get_model_size(int8_model)

    int8_top1, int8_top5, int8_time = evaluate_model(int8_model, val_loader, "cpu")

    results['Pretrained INT8'] = {'Model Size (MB)': int8_size, 'Top-1 Acc (%)': int8_top1, 'Top-5 Acc (%)': int8_top5, 'Inference Time (s)': int8_time}

    print(f"\n[Pretrained INT8] Model size: {int8_size:.2f} MB")
    print(f"[Pretrained INT8] Top-1 Accuracy: {int8_top1:.2f}%")
    print(f"[Pretrained INT8] Top-5 Accuracy: {int8_top5:.2f}%")
    print(f"[Pretrained INT8] Evaluation time (CPU): {int8_time:.2f} s")

except Exception as e:
    print(f"Failed to load Pretrained INT8 model: {e}. Skipping this step.")
    results['Pretrained INT8'] = {'Model Size (MB)': 'N/A', 'Top-1 Acc (%)': 'N/A', 'Top-5 Acc (%)': 'N/A', 'Inference Time (s)': 'N/A'}

# ==============================================================================
# Step 3: Manual INT8 Quantization (Naive implementation with scale)
# ==============================================================================
print("\n--- Step 3: Manual INT8 quantization and evaluation ---")

# Quantization: map float tensor → int8 tensor using max-abs scaling
def quantize_tensor(x):
    alpha = torch.max(torch.abs(x))        # max absolute value
    scale = alpha / 127.0                  # scaling factor
    x_q = torch.round(x / scale).to(torch.int8)
    return x_q, scale

# Dequantization: map int8 tensor → float tensor
def dequantize_tensor(x_q, scale):
    return x_q.float() * scale

manual_quant_model = copy.deepcopy(fp32_model).to("cpu")
quantized_state_dict = {}
scales = {}

# Apply quantization to weights
for name, param in manual_quant_model.state_dict().items():
    if 'weight' in name and param.dim() > 1:   # only quantize weight tensors
        quantized_param, scale = quantize_tensor(param)
        quantized_state_dict[name] = quantized_param
        scales[name] = scale
    else:
        quantized_state_dict[name] = param

# Estimate model size: count int8 params (1 byte each) and scales (float32)
quant_params_size = 0
float_params_size = 0
for name, param in quantized_state_dict.items():
    if param.dtype == torch.int8:
        quant_params_size += param.numel() * 1   # int8 → 1 byte
        float_params_size += 4                  # store scale (1 float32 per tensor)
    else:
        float_params_size += param.numel() * 4
manual_quant_size = (quant_params_size + float_params_size) / (1024*1024)

# Reconstruct model with dequantized parameters for evaluation
dequantized_state_dict = {}
for name, param in quantized_state_dict.items():
    if name in scales:
        dequantized_state_dict[name] = dequantize_tensor(param, scales[name])
    else:
        dequantized_state_dict[name] = param

eval_model = copy.deepcopy(fp32_model).to("cpu")
eval_model.load_state_dict(dequantized_state_dict)

manual_quant_top1, manual_quant_top5, manual_quant_time = evaluate_model(eval_model, val_loader, "cpu")

results['Manual INT8'] = {
    'Model Size (MB)': manual_quant_size,
    'Top-1 Acc (%)': manual_quant_top1,
    'Top-5 Acc (%)': manual_quant_top5,
    'Inference Time (s)': manual_quant_time
}

print(f"\n[Manual INT8] Model size: {manual_quant_size:.2f} MB")
print(f"[Manual INT8] Top-1 Accuracy: {manual_quant_top1:.2f}%")
print(f"[Manual INT8] Top-5 Accuracy: {manual_quant_top5:.2f}%")
print(f"[Manual INT8] Inference time (CPU): {manual_quant_time:.2f} s")

# ==============================================================================
# Step 4: Evaluate FX-based PTQ INT8
# ==============================================================================

print("\n--- Step 4: Start FX-based PTQ (INT8, W&A) ---")

import torch
from torch.ao.quantization import get_default_qconfig
from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx
from torchvision import models
from tqdm import tqdm

# 0) Engine setting (x86 CPU → fbgemm)
torch.backends.quantized.engine = "fbgemm"

# 1) Load FP32 model
fp32_fx = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1).eval()

# 2) qconfig_dict setting (same qconfig for all layers)
qconfig = get_default_qconfig(torch.backends.quantized.engine)
qconfig_dict = {"": qconfig}

# 3) FX preparation step (example_inputs required!)
example_inputs = torch.randn(1, 3, 224, 224)
prepared = prepare_fx(fp32_fx, qconfig_dict, example_inputs=example_inputs)

# 4) Calibration (pass a few batches from val_loader to collect observer stats)
print("--- Start calibration ---")
prepared.eval()
with torch.no_grad():
    for i, (images, _) in enumerate(tqdm(val_loader, desc="Calibrating", ncols=80)):
        images = images.to("cpu")
        prepared(images)
        if i >= 10:  # about 10 batches is enough
            break
print("--- Calibration completed ---")

# 5) Convert to INT8
int8_fx = convert_fx(prepared).eval()

# 6) Evaluate model size, accuracy, and inference time
ptq_size = get_model_size(int8_fx)
ptq_top1, ptq_top5, ptq_time = evaluate_model(int8_fx, val_loader, device="cpu")

results['PTQ (FX, INT8 W&A)'] = {
    'Model Size (MB)': ptq_size,
    'Top-1 Acc (%)': ptq_top1,
    'Top-5 Acc (%)': ptq_top5,
    'Inference Time (s)': ptq_time
}

print(f"\n[PTQ (FX, INT8 W&A)] Model size: {ptq_size:.2f} MB")
print(f"[PTQ (FX, INT8 W&A)] Top-1 Accuracy: {ptq_top1:.2f}%")
print(f"[PTQ (FX, INT8 W&A)] Top-5 Accuracy: {ptq_top5:.2f}%")
print(f"[PTQ (FX, INT8 W&A)] Evaluation time (CPU): {ptq_time:.2f} s")

# ==============================================================================
# Step 5: Final results comparison (table)
# ==============================================================================
import pandas as pd
from IPython.display import display, Markdown

print("\n--- Final Results Comparison ---\n")

df = pd.DataFrame(results).T.round(2)

display(Markdown("### Table 1: Model Performance Comparison"))
display(df)
