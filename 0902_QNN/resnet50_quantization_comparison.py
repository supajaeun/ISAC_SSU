# -*- coding: utf-8 -*-
"""QNN_0904_2118.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RmRUe5m3GCuHzMEIs-raZnKMZKf2UR-C
"""

!pip install torch torchvision torchaudio tqdm -q

# --- 라이브러리 임포트 ---
import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
from torch.utils.data import DataLoader
import os
import time
import copy
import numpy as np
from tqdm import tqdm
import pandas as pd

# --- GPU / CPU 설정 ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"사용 디바이스: {device}")
print(f"PyTorch 버전: {torch.__version__}")
print(f"NumPy 버전: {np.__version__}")

# --- Kaggle API 설정 (Kaggle.json 업로드 필요) ---
# 아래 셀 실행 후 나타나는 '파일 선택' 버튼을 눌러 kaggle.json 파일을 업로드하세요.
from google.colab import files
if not os.path.exists("kaggle.json"):
    files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# --- ImageNet-mini 데이터셋 다운로드 및 압축 해제 ---
if not os.path.exists("/content/imagenet-mini"):
    print("ImageNet-mini 데이터셋 다운로드를 시작합니다...")
    !kaggle datasets download -d ifigotin/imagenetmini-1000
    !unzip -q imagenetmini-1000.zip -d /content/imagenet-mini
    print("다운로드 및 압축 해제 완료.")
else:
    print("ImageNet-mini 데이터셋이 이미 존재합니다.")

# --- 데이터 로더 설정 ---
data_dir = "/content/imagenet-mini/imagenet-mini"
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
val_dataset = torchvision.datasets.ImageFolder(os.path.join(data_dir, "val"), transform=transform)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2) # 배치 사이즈 증가

print(f"\nValidation 이미지 개수: {len(val_dataset)}")
print(f"클래스 개수: {len(val_dataset.classes)}")

# 평가함수

def evaluate_model(model, dataloader, device):
    model.to(device)
    model.eval()
    correct_top1, correct_top5, total = 0, 0, 0
    start_time = time.time()
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Evaluating", ncols=80):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)

            # Top-5 예측값 계산
            _, pred = outputs.topk(5, 1, True, True)
            pred = pred.t()
            correct = pred.eq(labels.view(1, -1).expand_as(pred))

            # Top-1 정확도
            correct_top1 += correct[:1].reshape(-1).float().sum(0, keepdim=True).item()
            # Top-5 정확도
            correct_top5 += correct[:5].reshape(-1).float().sum(0, keepdim=True).item()
            total += labels.size(0)

    elapsed = time.time() - start_time
    top1_acc = (correct_top1 / total) * 100
    top5_acc = (correct_top5 / total) * 100
    return top1_acc, top5_acc, elapsed

# --- 공통 모델 크기 측정 함수 ---
def get_model_size(model, file_path="temp_model.pth"):
    torch.save(model.state_dict(), file_path)
    size_mb = os.path.getsize(file_path) / (1024 * 1024)
    os.remove(file_path)
    return size_mb

# 결과 저장을 위한 딕셔너리
results = {}

# ==============================================================================
# ## 1단계: Pretrained FP32 ResNet-50 성능 측정 (CPU 평가)
# ==============================================================================
print("\n--- 1단계: FP32 ResNet-50 성능 측정 시작 ---")
fp32_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)

fp32_size = get_model_size(fp32_model)

# =============================================================================
#  수정된 부분: device 변수 대신 "cpu"를 직접 전달하여 CPU에서 평가
# =============================================================================
fp32_top1, fp32_top5, fp32_time = evaluate_model(fp32_model, val_loader, "cpu")


results['FP32'] = {'Model Size (MB)': fp32_size, 'Top-1 Acc (%)': fp32_top1, 'Top-5 Acc (%)': fp32_top5, 'Inference Time (s)': fp32_time}

print(f"\n[FP32] 모델 크기: {fp32_size:.2f} MB")
print(f"[FP32] Top-1 정확도: {fp32_top1:.2f}%")
print(f"[FP32] Top-5 정확도: {fp32_top5:.2f}%")
print(f"[FP32] 평가 시간 (CPU): {fp32_time:.2f} 초")

# ==============================================================================
# ## 2단계: Pretrained INT8 ResNet-50 (Quantized) 성능 측정
# ==============================================================================
print("\n--- 2단계: Pretrained INT8 ResNet-50 성능 측정 시작 ---")
try:
    int8_model = torchvision.models.quantization.resnet50(pretrained=True, quantize=True)
    int8_model.eval()

    int8_size = get_model_size(int8_model)

    int8_top1, int8_top5, int8_time = evaluate_model(int8_model, val_loader, "cpu")

    results['Pretrained INT8'] = {'Model Size (MB)': int8_size, 'Top-1 Acc (%)': int8_top1, 'Top-5 Acc (%)': int8_top5, 'Inference Time (s)': int8_time}

    print(f"\n[Pretrained INT8] 모델 크기: {int8_size:.2f} MB")
    print(f"[Pretrained INT8] Top-1 정확도: {int8_top1:.2f}%")
    print(f"[Pretrained INT8] Top-5 정확도: {int8_top5:.2f}%")
    print(f"[Pretrained INT8] 평가 시간 (CPU): {int8_time:.2f} 초")

except Exception as e:
    print(f"Pretrained INT8 모델 로딩 실패: {e}. 이 단계는 건너뜁니다.")
    results['Pretrained INT8'] = {'Model Size (MB)': 'N/A', 'Top-1 Acc (%)': 'N/A', 'Top-5 Acc (%)': 'N/A', 'Inference Time (s)': 'N/A'}

# ==============================================================================
# ## 3단계: FP32 ResNet-50 직접 양자화 구현 (수식 기반)
# ==============================================================================
print("\n--- 3단계: 수식 기반 직접 양자화 구현 및 성능 측정 시작 ---")

def quantize_tensor(x):
    alpha = torch.max(torch.abs(x))
    scale = alpha / 127.0
    x_q = torch.round(x / scale).to(torch.int8)
    return x_q, scale

def dequantize_tensor(x_q, scale):
    return x_q.float() * scale

manual_quant_model = copy.deepcopy(fp32_model).to("cpu")
quantized_state_dict = {}
scales = {}

for name, param in manual_quant_model.state_dict().items():
    if 'weight' in name and param.dim() > 1:
        quantized_param, scale = quantize_tensor(param)
        quantized_state_dict[name] = quantized_param
        scales[name] = scale
    else:
        quantized_state_dict[name] = param

quant_params_size = 0
float_params_size = 0
for name, param in quantized_state_dict.items():
    if param.dtype == torch.int8:
        quant_params_size += param.numel() * 1
        float_params_size += 4
    else:
        float_params_size += param.numel() * 4

manual_quant_size = (quant_params_size + float_params_size) / (1024*1024)

dequantized_state_dict = {}
for name, param in quantized_state_dict.items():
    if name in scales:
        dequantized_state_dict[name] = dequantize_tensor(param, scales[name])
    else:
        dequantized_state_dict[name] = param

eval_model = copy.deepcopy(fp32_model).to("cpu")
eval_model.load_state_dict(dequantized_state_dict)

# =============================================================================
#  수정된 부분: device 변수 대신 "cpu"를 직접 전달하여 CPU에서 평가
# =============================================================================
manual_quant_top1, manual_quant_top5, manual_quant_time = evaluate_model(eval_model, val_loader, "cpu")

results['Manual INT8'] = {'Model Size (MB)': manual_quant_size, 'Top-1 Acc (%)': manual_quant_top1, 'Top-5 Acc (%)': manual_quant_top5, 'Inference Time (s)': manual_quant_time}

print(f"\n[Manual INT8] 모델 크기: {manual_quant_size:.2f} MB")
print(f"[Manual INT8] Top-1 정확도: {manual_quant_top1:.2f}%")
print(f"[Manual INT8] Top-5 정확도: {manual_quant_top5:.2f}%")
print(f"[Manual INT8] 평가 시간 (CPU): {manual_quant_time:.2f} 초")

# ==============================================================================
# ## 4단계: 최종 결과 비교 및 분석 (테이블)
# ==============================================================================
import pandas as pd
from IPython.display import display, Markdown # display 함수를 임포트합니다.

print("\n--- Final Results Comparison ---\n")

df = pd.DataFrame(results).T.round(2)

# print(df) 대신 display(df)를 사용하여 명시적으로 표를 렌더링합니다.
# Markdown을 이용해 제목을 추가할 수도 있습니다.
display(Markdown("### Table 1: Model Performance Comparison"))
display(df)